{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a8b5e0",
   "metadata": {},
   "source": [
    "# Enhanced patient trial matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c66ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mgustineli/github/llm-drug-discovery'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Get project directory\n",
    "def get_project_dir():\n",
    "    root = Path(os.path.expanduser(\"~\"))\n",
    "    return f\"{root}/github/llm-drug-discovery\"\n",
    "\n",
    "\n",
    "project_dir = get_project_dir()\n",
    "project_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcaf7c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Project directories\n",
    "PLOT_DIR = f\"{project_dir}/plots\"\n",
    "\n",
    "# Hugging Face authentication from `.env`\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HF_TOKEN not found in environment (.env).\")\n",
    "login(token=hf_token)\n",
    "\n",
    "# Determinism for reproducibility\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afcb4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):  # tiny helper\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "train_patient_data = read_json(f\"{project_dir}/data/train_patients.json\")\n",
    "train_trial_data = read_json(f\"{project_dir}/data/train_trials.json\")\n",
    "val_patient_data = read_json(f\"{project_dir}/data/test_patients.json\")\n",
    "val_trial_data = read_json(f\"{project_dir}/data/test_trials.json\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "base_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98afd2",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7021b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientTrialDataset(Dataset):\n",
    "    \"\"\"Creates balanced +/- pairs for a single clinical trial.\"\"\"\n",
    "\n",
    "    def __init__(self, patient_data, trial, tokenizer, max_length=512, neg_ratio=1.0):\n",
    "        self.trial = trial\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        elig = set(trial[\"eligible_patients\"])\n",
    "        pos = [p for p in patient_data if p[\"id\"] in elig]\n",
    "        neg = [p for p in patient_data if p[\"id\"] not in elig]\n",
    "        nneg = min(int(len(pos) * neg_ratio), len(neg))\n",
    "        self.samples = [(p, 1) for p in pos] + [\n",
    "            (p, 0) for p in np.random.choice(neg, nneg, replace=False)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient, label = self.samples[idx]\n",
    "\n",
    "        ptxt = f\"Demographics: {patient['demographics']}. Medical History: {patient['medical_history']}\"\n",
    "        ttxt = (\n",
    "            f\"Trial: {self.trial['name']}. Type: {self.trial['type']}. \"\n",
    "            f\"Description: {self.trial['description']}. \"\n",
    "            f\"Eligibility Criteria: {self.trial['eligibility_criteria']['text']}\"\n",
    "        )\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            ptxt,\n",
    "            ttxt,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "def make_dataloader(\n",
    "    patients,\n",
    "    trials,\n",
    "    tokenizer,\n",
    "    max_length=512,\n",
    "    neg_ratio=1.0,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "):\n",
    "    \"\"\"Returns DataLoader over concatenated datasets across trials.\"\"\"\n",
    "    datasets = [\n",
    "        PatientTrialDataset(\n",
    "            patients, trial, tokenizer, max_length=max_length, neg_ratio=neg_ratio\n",
    "        )\n",
    "        for trial in trials\n",
    "    ]\n",
    "    concat = torch.utils.data.ConcatDataset(datasets)\n",
    "    return DataLoader(\n",
    "        concat,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "\n",
    "train_loader = make_dataloader(\n",
    "    train_patient_data,\n",
    "    train_trial_data,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "val_loader = make_dataloader(\n",
    "    val_patient_data,\n",
    "    val_trial_data,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a040a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad316da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "\n",
    "class PatientTrialModule(pl.LightningModule):\n",
    "    def __init__(self, bert, lr=2e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"bert\"])\n",
    "        self.bert = bert\n",
    "        hidden = bert.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 2),\n",
    "        )\n",
    "        self.acc = torchmetrics.Accuracy(task=\"binary\", num_classes=2)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        pooled = self.bert(ids, attention_mask=mask).pooler_output\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "    # training / validation\n",
    "    def step(self, batch):\n",
    "        logits = self(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        loss = nn.functional.cross_entropy(logits, batch[\"labels\"])\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.acc(preds, batch[\"labels\"])\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        loss, acc = self.step(batch)\n",
    "        self.log_dict({\"train_loss\": loss, \"train_acc\": acc}, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        loss, acc = self.step(batch)\n",
    "        self.log_dict({\"val_loss\": loss, \"val_acc\": acc}, prog_bar=True)\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "\n",
    "    # def validation_epoch_end(self, outs):\n",
    "    #     outs_stack = {k: torch.stack([o[k] for o in outs]).mean() for k in outs[0]}\n",
    "    #     self.log_dict({f\"epoch_{k}\": v for k, v in outs_stack.items()})\n",
    "\n",
    "    # optim\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.hparams.lr, weight_decay=0.01\n",
    "        )\n",
    "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            opt, mode=\"min\", patience=2, factor=0.5\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": opt,\n",
    "            \"lr_scheduler\": {\"scheduler\": sch, \"monitor\": \"val_loss\"},\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1a613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgustineli/github/llm-drug-discovery/.venv/lib/python3.10/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "/home/mgustineli/github/llm-drug-discovery/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:513: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/mgustineli/github/llm-drug-discovery/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name       | Type           | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | bert       | BertModel      | 108 M  | eval \n",
      "1 | classifier | Sequential     | 213 K  | train\n",
      "2 | acc        | BinaryAccuracy | 0      | train\n",
      "------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "434.095   Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba111142fa545e09c630bebc4497d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgustineli/github/llm-drug-discovery/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n",
      "/home/mgustineli/github/llm-drug-discovery/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabd2685ce184bcca3afda2493f87600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train\n",
    "model = PatientTrialModule(base_model)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2)],\n",
    "    deterministic=True,\n",
    "    log_every_n_steps=10,\n",
    "    # precision=16,\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33fa1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to match patients with clinical trials\n",
    "def match_patients_with_trials(\n",
    "    model, patient_data, trial_data, tokenizer, threshold=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Match patients with clinical trials using the trained model\n",
    "    Returns a dictionary of trial_id -> list of matched patient_ids\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    matches = {}\n",
    "\n",
    "    for trial in trial_data:\n",
    "        trial_id = trial[\"id\"]\n",
    "        matches[trial_id] = []\n",
    "\n",
    "        for patient in patient_data:\n",
    "            patient_id = patient[\"id\"]\n",
    "\n",
    "            # Create patient text\n",
    "            patient_text = f\"Demographics: {patient['demographics']}. \"\n",
    "            patient_text += f\"Medical History: {patient['medical_history']}\"\n",
    "\n",
    "            # Create trial text\n",
    "            trial_text = f\"Trial: {trial['name']}. Type: {trial['type']}. \"\n",
    "            trial_text += f\"Description: {trial['description']}. \"\n",
    "            trial_text += (\n",
    "                f\"Eligibility Criteria: {trial['eligibility_criteria']['text']}\"\n",
    "            )\n",
    "\n",
    "            # Tokenize\n",
    "            encoding = tokenizer.encode_plus(\n",
    "                patient_text,\n",
    "                trial_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            # Get prediction\n",
    "            with torch.no_grad():\n",
    "                input_ids = encoding[\"input_ids\"]\n",
    "                attention_mask = encoding[\"attention_mask\"]\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                match_probability = probabilities[0][\n",
    "                    1\n",
    "                ].item()  # Probability of class 1 (match)\n",
    "\n",
    "                # If probability exceeds threshold, consider it a match\n",
    "                if match_probability >= threshold:\n",
    "                    matches[trial_id].append(\n",
    "                        {\n",
    "                            \"patient_id\": patient_id,\n",
    "                            \"match_probability\": match_probability,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04837394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate matching performance\n",
    "def evaluate_matching(matches, trial_data):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the matching algorithm\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"trial_id\": [],\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1\": [],\n",
    "        \"num_predicted\": [],\n",
    "        \"num_actual\": [],\n",
    "        \"num_correct\": [],\n",
    "    }\n",
    "\n",
    "    for trial in trial_data:\n",
    "        trial_id = trial[\"id\"]\n",
    "        actual_matches = set(trial[\"eligible_patients\"])\n",
    "        predicted_matches = set([m[\"patient_id\"] for m in matches.get(trial_id, [])])\n",
    "\n",
    "        # Calculate metrics\n",
    "        correct_matches = actual_matches.intersection(predicted_matches)\n",
    "\n",
    "        precision = (\n",
    "            len(correct_matches) / len(predicted_matches) if predicted_matches else 0\n",
    "        )\n",
    "        recall = len(correct_matches) / len(actual_matches) if actual_matches else 1.0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        results[\"trial_id\"].append(trial_id)\n",
    "        results[\"precision\"].append(precision)\n",
    "        results[\"recall\"].append(recall)\n",
    "        results[\"f1\"].append(f1)\n",
    "        results[\"num_predicted\"].append(len(predicted_matches))\n",
    "        results[\"num_actual\"].append(len(actual_matches))\n",
    "        results[\"num_correct\"].append(len(correct_matches))\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_precision = np.mean(results[\"precision\"])\n",
    "    overall_recall = np.mean(results[\"recall\"])\n",
    "    overall_f1 = np.mean(results[\"f1\"])\n",
    "\n",
    "    print(f\"Overall Precision: {overall_precision:.4f}\")\n",
    "    print(f\"Overall Recall: {overall_recall:.4f}\")\n",
    "    print(f\"Overall F1 Score: {overall_f1:.4f}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Plot training metrics\n",
    "model.plot_metrics()\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "matches = match_patients_with_trials(\n",
    "    model, test_patient_data, test_trial_data, tokenizer\n",
    ")\n",
    "evaluation_df = evaluate_matching(matches, test_trial_data)\n",
    "\n",
    "# Save evaluation results\n",
    "evaluation_df.to_csv(f\"{project_dir}/data/matching_evaluation.csv\", index=False)\n",
    "\n",
    "# Visualize evaluation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"trial_id\", y=\"f1\", data=evaluation_df)\n",
    "plt.title(\"F1 Score by Trial\")\n",
    "plt.xlabel(\"Trial ID\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{project_dir}/plots/f1_by_trial.png\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"f{project_dir}/data/patient_trial_matching_model.pt\")\n",
    "\n",
    "print(\n",
    "    \"Evaluation complete! Results saved to {fproject_dir}/data/matching_evaluation.csv\"\n",
    ")\n",
    "print(\"Visualizations saved to {fproject_dir}/plots/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
